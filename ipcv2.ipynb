{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V5E1","authorship_tag":"ABX9TyMwBDsD2ntLnelhcZ4tXu+S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["# ==============================================================\n","# FINAL BCM (FSS-1000 style) — FIXED: prediction -> single-channel\n","# - Works with dataset.zip\n","# - Ensures pred masks are single-channel (shape HxW)\n","# - Prints co-occurrence, BNM, per-class IoU, summary\n","# - Pretty confusion matrix + heatmap plot\n","# - Saves CSVs under OUT_DIR/csv/ and preds under OUT_DIR/visuals/\n","# ==============================================================\n","\n","import os, zipfile, random, csv, json, traceback\n","from collections import defaultdict, Counter\n","import numpy as np\n","from PIL import Image, ImageDraw\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from torchvision import models, transforms\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.utils import resample\n","from sklearn.metrics import confusion_matrix\n","\n","import matplotlib.pyplot as plt\n","\n","# -----------------------------\n","# USER CONFIG\n","# -----------------------------\n","ZIP_PATH    = \"/content/JPEGImages.zip\"   # upload your zip at /content/dataset.zip\n","EXTRACT_DIR = \"/content/FSS1000\"\n","OUT_DIR     = \"/content/bcm_fss1000_outputs\"\n","\n","NUM_NOVEL = 3\n","K_SHOT = 5\n","IMG_SIZE = 256\n","TOP_S = 1\n","TAU = 0.5\n","MAX_PIXELS_PER_CLASS_PER_IMAGE = 400\n","\n","RANDOM_SEED = 0\n","PREFER_CUDA = False\n","\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n","# -----------------------------\n","CSV_DIR = os.path.join(OUT_DIR, \"csv\")\n","VIS_DIR = os.path.join(OUT_DIR, \"visuals\")\n","def safe_mkdir(d): os.makedirs(d, exist_ok=True)\n","\n","# ==============================================================\n","# Utility: Confusion matrix heatmap (as requested)\n","# ==============================================================\n","def plot_confusion_matrix(cm, class_labels, title=\"Confusion Matrix\"):\n","    plt.figure(figsize=(7, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","    plt.title(title, fontsize=16)\n","    plt.colorbar()\n","\n","    ticks = np.arange(len(class_labels))\n","    plt.xticks(ticks, class_labels, rotation=45, ha=\"right\")\n","    plt.yticks(ticks, class_labels)\n","\n","    thresh = cm.max() / 2.0 if cm.size else 0\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(\n","                j, i, str(int(cm[i, j])),\n","                horizontalalignment=\"center\",\n","                color=\"white\" if cm[i, j] > thresh else \"black\",\n","                fontsize=11\n","            )\n","\n","    plt.ylabel(\"Actual\", fontsize=14)\n","    plt.xlabel(\"Predicted\", fontsize=14)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# ==============================================================\n","# Dataset helpers\n","# ==============================================================\n","def extract_zip(zip_path, target_dir):\n","    if os.path.isdir(target_dir) and any(os.scandir(target_dir)):\n","        print(\"[info] dataset already extracted:\", target_dir)\n","        return\n","    if not os.path.exists(zip_path):\n","        print(f\"[error] ZIP not found: {zip_path}\")\n","        return\n","    if not zipfile.is_zipfile(zip_path):\n","        print(f\"[error] INVALID ZIP: {zip_path}\")\n","        return\n","    print(\"[info] extracting zip...\")\n","    with zipfile.ZipFile(zip_path, 'r') as z:\n","        z.extractall(target_dir)\n","    print(\"[info] extraction done.\")\n","\n","def find_class_folders(root):\n","    classes=[]\n","    for entry in sorted(os.listdir(root)):\n","        p = os.path.join(root, entry)\n","        if os.path.isdir(p):\n","            img_files=[f for f in os.listdir(p) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n","            if img_files:\n","                classes.append((entry, p))\n","    return classes\n","\n","def list_images(folder):\n","    return sorted([f for f in os.listdir(folder) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))])\n","\n","def find_mask_in_class(folder, image_name):\n","    base=os.path.splitext(image_name)[0]\n","    candidates=[\n","        base+\".png\", base+\".jpg\", base+\".jpeg\",\n","        base+\"_mask.png\", base+\"_mask.jpg\",\n","        base+\"mask.png\", base+\"-mask.png\"\n","    ]\n","    for c in candidates:\n","        p=os.path.join(folder,c)\n","        if os.path.exists(p): return p\n","    for f in os.listdir(folder):\n","        if base in f and \"mask\" in f.lower():\n","            return os.path.join(folder,f)\n","    return None\n","\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n","    transforms.ToTensor()\n","])\n","\n","def load_image_tensor(path, device):\n","    return transform(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(device)\n","\n","def load_mask(path):\n","    return np.array(Image.open(path).resize((IMG_SIZE,IMG_SIZE), Image.NEAREST))\n","\n","# ==============================================================\n","# Model loading & features\n","# ==============================================================\n","def get_device():\n","    if PREFER_CUDA and torch.cuda.is_available():\n","        return \"cuda\"\n","    return \"cpu\"\n","\n","def load_base_segmentation_model(device):\n","    try:\n","        m=models.segmentation.deeplabv3_resnet50(pretrained=True)\n","        m.eval().to(device)\n","        print(\"[info] Loaded DeepLabV3\")\n","        return m\n","    except Exception as e:\n","        print(\"[warn] Failed to load DeepLabV3:\", e)\n","        return None\n","\n","def load_backbone(device):\n","    m=models.mobilenet_v2(pretrained=True).features\n","    m.eval().to(device)\n","    return m\n","\n","def get_feature_map(backbone, img):\n","    with torch.no_grad():\n","        fm = backbone(img)\n","    return fm.squeeze(0).cpu().numpy()  # (C,Hf,Wf)\n","\n","# ==============================================================\n","# BCM core\n","# ==============================================================\n","def tukey(X, tau):\n","    return np.sign(X) * (np.abs(X) ** tau)\n","\n","def normalize_mask_to_class(mask_arr, class_name):\n","    # For FSS1000: non-zero foreground -> class_id\n","    cid = class_to_id[class_name]\n","    out = np.zeros_like(mask_arr, dtype=int)\n","    out[mask_arr != 0] = cid\n","    return out\n","\n","def compute_cooccurrence(support_pairs, base_model, backbone, device):\n","    cooc = Counter()\n","    for img_p, mask_p in tqdm(support_pairs, desc=\"Co-occurrence\"):\n","        try:\n","            cls = os.path.basename(os.path.dirname(img_p))\n","            img_t = load_image_tensor(img_p, device)\n","            gt = normalize_mask_to_class(load_mask(mask_p), cls)\n","\n","            if base_model is not None:\n","                with torch.no_grad():\n","                    out = base_model(img_t)['out']\n","                    out = F.interpolate(out, size=(IMG_SIZE,IMG_SIZE), mode='bilinear', align_corners=False)\n","                    bp = out.argmax(1).squeeze(0).cpu().numpy().astype(int)\n","            else:\n","                fm = get_feature_map(backbone, img_t)\n","                bp = fm.argmax(0).astype(int)\n","                bp = np.array(Image.fromarray(bp.astype(np.uint8)).resize((IMG_SIZE,IMG_SIZE), Image.NEAREST))\n","\n","            for nid in novel_ids:\n","                mask_idx = (gt == nid)\n","                if mask_idx.any():\n","                    uniq, counts = np.unique(bp[mask_idx], return_counts=True)\n","                    for b, c in zip(uniq, counts):\n","                        cooc[(int(b), int(nid))] += int(c)\n","        except Exception:\n","            print(\"[warn] skipping support image due to error:\", img_p)\n","            traceback.print_exc()\n","    return cooc\n","\n","def build_bnm_from_cooc(cooc, top_s=TOP_S):\n","    novel_to_bases = {}\n","    for n in novel_ids:\n","        pairs = [(cnt,b) for ((b,nn),cnt) in cooc.items() if nn==n]\n","        pairs.sort(reverse=True)\n","        novel_to_bases[n] = [b for _,b in pairs[:top_s]]\n","    base_to_novels = defaultdict(list)\n","    for n, bl in novel_to_bases.items():\n","        for b in bl: base_to_novels[b].append(n)\n","    B = set(base_to_novels.keys())\n","    return novel_to_bases, dict(base_to_novels), B\n","\n","def sample_pixels_for_training(fm, mask_small, mapped_novels, beta_label, max_per_class=MAX_PIXELS_PER_CLASS_PER_IMAGE):\n","    C,Hf,Wf = fm.shape\n","    flat_feats = fm.reshape(C, -1).T\n","    flat_mask = mask_small.reshape(-1)\n","    mapped = np.where(np.isin(flat_mask, mapped_novels), flat_mask, beta_label)\n","    Xs, ys = [], []\n","    for lab in np.unique(mapped):\n","        idxs = np.where(mapped==lab)[0]\n","        if len(idxs) == 0: continue\n","        n = min(len(idxs), max_per_class)\n","        chosen = resample(idxs, replace=False, n_samples=n, random_state=RANDOM_SEED)\n","        Xs.append(flat_feats[chosen]); ys.append(np.full(len(chosen), lab))\n","    if not Xs:\n","        return np.zeros((0, C)), np.zeros((0,), dtype=int)\n","    return np.vstack(Xs), np.concatenate(ys)\n","\n","def train_models_for_B(support_pairs, backbone, base_to_novels, device):\n","    models = {}\n","    problems = {}\n","    for beta, mapped in base_to_novels.items():\n","        print(f\"[train] base {beta} -> mapped novels {mapped}\")\n","        X_all, y_all = [], []\n","        for img_p, mask_p in support_pairs:\n","            try:\n","                img_t = load_image_tensor(img_p, device)\n","                fm = get_feature_map(backbone, img_t)\n","                cls = os.path.basename(os.path.dirname(img_p))\n","                arr = normalize_mask_to_class(load_mask(mask_p), cls)\n","                Hf,Wf = fm.shape[1], fm.shape[2]\n","                mask_small = np.array(Image.fromarray(arr).resize((Wf,Hf), Image.NEAREST))\n","                X, y = sample_pixels_for_training(fm, mask_small, mapped, beta)\n","                if X.shape[0] > 0:\n","                    X_all.append(X); y_all.append(y)\n","            except Exception:\n","                print(\"[warn] skipping image during training:\", img_p)\n","                traceback.print_exc()\n","        if not X_all:\n","            print(f\"[warn] no training pixels for base {beta} -> skipping model\")\n","            problems[beta] = \"no_data\"\n","            continue\n","        X_all = np.vstack(X_all); y_all = np.concatenate(y_all)\n","        if len(np.unique(y_all)) < 2:\n","            print(f\"[warn] only one class present in training data for base {beta}. skipping\")\n","            problems[beta] = \"single_class\"\n","            continue\n","        Xp = tukey(np.abs(X_all) + 1e-8, TAU)\n","        try:\n","            clf = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=300)\n","            clf.fit(Xp, y_all)\n","            models[beta] = {'clf': clf, 'tau': TAU}\n","        except Exception as e:\n","            print(f\"[error] training failed for base {beta}: {e}\")\n","            problems[beta] = f\"train_error:{e}\"\n","    return models, problems\n","\n","def bcm_inference_on_image(img_p, base_model, backbone, models_B, B, device):\n","    img_t = load_image_tensor(img_p, device)\n","\n","    if base_model is not None:\n","        with torch.no_grad():\n","            out = base_model(img_t)['out']\n","            out = F.interpolate(out, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n","            base_pred = out.argmax(1).squeeze(0).cpu().numpy().astype(int)\n","        fm_tmp = get_feature_map(backbone, img_t)\n","        Hf,Wf = fm_tmp.shape[1], fm_tmp.shape[2]\n","        base_pred_small = np.array(Image.fromarray(base_pred.astype(np.uint8)).resize((Wf,Wf), Image.NEAREST))\n","        # Note: resized to (Wf,Wf) was a bug risk; use (Wf,Hf) -> correct:\n","        base_pred_small = np.array(Image.fromarray(base_pred.astype(np.uint8)).resize((Wf,Hf), Image.NEAREST)).astype(int)\n","    else:\n","        fm_tmp = get_feature_map(backbone, img_t)\n","        base_pred_small = fm_tmp.argmax(0).astype(int)\n","\n","    fm = get_feature_map(backbone, img_t)\n","    C = fm.shape[0]\n","    flat_feats = fm.reshape(C, -1).T\n","    final = base_pred_small.reshape(-1).copy()\n","\n","    for beta, entry in models_B.items():\n","        if beta not in B: continue\n","        idxs = np.where(final == beta)[0]\n","        if idxs.size == 0: continue\n","        Xpix = flat_feats[idxs]\n","        Xp = tukey(np.abs(Xpix) + 1e-8, entry['tau'])\n","        try:\n","            preds = entry['clf'].predict(Xp)\n","            final[idxs] = preds\n","        except Exception as e:\n","            print(\"[warn] prediction failed for beta\", beta, \"-> leaving base predictions for those pixels. error:\", e)\n","\n","    final_small = final.reshape(fm.shape[1], fm.shape[2]).astype(int)\n","    final_up_pil = Image.fromarray(final_small.astype(np.uint8)).resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n","    final_up = np.array(final_up_pil).astype(np.uint8)\n","\n","    # --- PATCH: ensure single-channel 2D mask (H,W) ---\n","    if final_up.ndim == 3:\n","        # if shape (H,W,3) take first channel\n","        final_up = final_up[..., 0]\n","    final_up = np.squeeze(final_up)\n","    # ensure integer dtype\n","    final_up = final_up.astype(np.uint8)\n","    return final_up\n","\n","# ==============================================================\n","# Metrics and CSV helpers\n","# ==============================================================\n","def compute_class_iou(gt, pred, cid):\n","    # Ensure both are 2D and same shape\n","    if pred.ndim == 3:\n","        pred = pred[..., 0]\n","    if gt.ndim == 3:\n","        gt = gt[..., 0]\n","    if gt.shape != pred.shape:\n","        # Try resizing pred to gt shape if sensible\n","        try:\n","            pred = np.array(Image.fromarray(pred).resize((gt.shape[1], gt.shape[0]), Image.NEAREST))\n","        except Exception:\n","            raise ValueError(f\"GT shape {gt.shape} and pred shape {pred.shape} incompatible\")\n","\n","    gt_mask = (gt == cid)\n","    pr_mask = (pred == cid)\n","    inter = np.logical_and(gt_mask, pr_mask).sum()\n","    union = np.logical_or(gt_mask, pr_mask).sum()\n","    if union == 0:\n","        return None\n","    return inter / union\n","\n","def save_csv(path, rows):\n","    with open(path, 'w', newline='') as f:\n","        w = csv.writer(f)\n","        w.writerows(rows)\n","    print(\"[CSV saved]\", path)\n","\n","def pretty_print_confusion(cm, labels):\n","    print(\"\\n===== CONFUSION MATRIX (Pretty Option B) =====\")\n","    header = \"      \" + \"\".join([f\"{l:>8}\" for l in labels])\n","    print(header)\n","    for cid, row in zip(labels, cm):\n","        print(f\"{cid:>6} \" + \"\".join(f\"{v:>8}\" for v in row))\n","    print(\"=============================================\\n\")\n","\n","# ==============================================================\n","# Main pipeline\n","# ==============================================================\n","def main():\n","    safe_mkdir(OUT_DIR); safe_mkdir(CSV_DIR); safe_mkdir(VIS_DIR)\n","\n","    extract_zip(ZIP_PATH, EXTRACT_DIR)\n","\n","    # find class folders\n","    class_folders = find_class_folders(EXTRACT_DIR)\n","    if not class_folders:\n","        print(\"[error] no class folders found under EXTRACT_DIR:\", EXTRACT_DIR)\n","        return\n","\n","    print(f\"[info] detected {len(class_folders)} candidate class folders\")\n","\n","    # map to ids\n","    global class_to_id, id_to_class\n","    class_to_id = {name: i+1 for i, (name, _) in enumerate(class_folders)}\n","    id_to_class = {v:k for k,v in class_to_id.items()}\n","\n","    # pick novel classes\n","    class_names = list(class_to_id.keys())\n","    novel_class_names = random.sample(class_names, min(NUM_NOVEL, len(class_names)))\n","\n","    global novel_ids\n","    novel_ids = set(class_to_id[n] for n in novel_class_names)\n","    base_ids = set(class_to_id.values()) - novel_ids\n","\n","    print(\"[info] found classes:\", len(class_to_id))\n","    print(\"[info] novel classes chosen (names):\", novel_class_names)\n","    print(\"[info] novel IDs:\", novel_ids)\n","\n","    # build dataset_by_class -> list of (img_path, mask_path)\n","    dataset_by_class = {}\n","    for cname, cpath in class_folders:\n","        imgs = list_images(cpath)\n","        pairs = []\n","        for fn in imgs:\n","            img_p = os.path.join(cpath, fn)\n","            mask_p = find_mask_in_class(cpath, fn)\n","            if mask_p:\n","                pairs.append((img_p, mask_p))\n","        if pairs:\n","            dataset_by_class[cname] = pairs\n","\n","    # support selection K-shot per novel class\n","    support_pairs = []\n","    for cname in novel_class_names:\n","        pairs = dataset_by_class.get(cname, [])\n","        if not pairs:\n","            print(f\"[warn] no images for novel class {cname}\")\n","            continue\n","        if len(pairs) <= K_SHOT:\n","            chosen = pairs\n","        else:\n","            chosen = random.sample(pairs, K_SHOT)\n","        support_pairs.extend(chosen)\n","    support_pairs = list(dict.fromkeys(support_pairs))\n","    support_set = set([p[0] for p in support_pairs])\n","\n","    # query set = all others\n","    query_pairs = [p for cname,pairs in dataset_by_class.items() for p in pairs if p[0] not in support_set]\n","\n","    print(\"[info] support size:\", len(support_pairs), \"query size:\", len(query_pairs))\n","\n","    device = get_device()\n","    print(\"[info] device =\", device)\n","    base_model = load_base_segmentation_model(device)\n","    backbone = load_backbone(device)\n","\n","    # co-occurrence\n","    cooc = compute_cooccurrence(support_pairs, base_model, backbone, device)\n","    cooc_rows = [[\"base_class\",\"novel_class\",\"count\"]]\n","    for (b,n), cnt in sorted(cooc.items(), key=lambda x:-x[1]):\n","        cooc_rows.append([int(b), int(n), int(cnt)])\n","    save_csv(os.path.join(CSV_DIR, \"cooccurrence.csv\"), cooc_rows)\n","    print(\"\\n===== CO-OCCURRENCE TABLE =====\")\n","    for r in cooc_rows[:50]:\n","        print(\"\\t\".join(map(str,r)))\n","    print(\"================================\\n\")\n","\n","    # BNM\n","    novel_to_bases, base_to_novels, B = build_bnm_from_cooc(cooc, TOP_S)\n","    bnm_rows = [[\"novel\",\"mapped_bases\"]]\n","    for n in novel_ids:\n","        bnm_rows.append([int(n), novel_to_bases.get(n, [])])\n","    save_csv(os.path.join(CSV_DIR, \"bnm_mapping.csv\"), bnm_rows)\n","    print(\"BNM mapping (ids):\", novel_to_bases)\n","    print(\"BNM mapping (names):\", {id_to_class.get(n,str(n)): [id_to_class.get(b,str(b)) for b in bs] for n,bs in novel_to_bases.items()})\n","    print(\"B set:\", B)\n","\n","    # train models\n","    models_B, train_problems = train_models_for_B(support_pairs, backbone, base_to_novels, device)\n","    if train_problems:\n","        print(\"[warn] train problems:\", train_problems)\n","\n","    # inference & evaluation\n","    all_gt = []\n","    all_pred = []\n","    per_image_rows = [[\"image\",\"base_mIoU\",\"novel_mIoU\"]]\n","\n","    for img_p, mask_p in tqdm(query_pairs, desc=\"Inference & Eval\"):\n","        try:\n","            cls = os.path.basename(os.path.dirname(img_p))\n","            gt_raw = load_mask(mask_p)\n","            gt_mapped = normalize_mask_to_class(gt_raw, cls)\n","\n","            pred_up = bcm_inference_on_image(img_p, base_model, backbone, models_B, B, device)\n","\n","            # SAFETY: Ensure pred_up is 2D (H,W)\n","            if pred_up.ndim == 3:\n","                pred_up = pred_up[...,0]\n","            pred_up = np.squeeze(pred_up).astype(np.uint8)\n","\n","            # ensure gt and pred shapes match (if not, resize pred to gt)\n","            if gt_mapped.shape != pred_up.shape:\n","                pred_up = np.array(Image.fromarray(pred_up).resize((gt_mapped.shape[1], gt_mapped.shape[0]), Image.NEAREST)).astype(np.uint8)\n","\n","            all_gt.append(gt_mapped)\n","            all_pred.append(pred_up)\n","\n","            # per-image mIoU\n","            base_vals = []\n","            for c in base_ids:\n","                v = compute_class_iou(gt_mapped, pred_up, c)\n","                if v is not None: base_vals.append(v)\n","            novel_vals = []\n","            for c in novel_ids:\n","                v = compute_class_iou(gt_mapped, pred_up, c)\n","                if v is not None: novel_vals.append(v)\n","            base_miou = float(np.mean(base_vals)) if base_vals else 0.0\n","            novel_miou = float(np.mean(novel_vals)) if novel_vals else 0.0\n","            per_image_rows.append([os.path.basename(img_p), base_miou, novel_miou])\n","\n","            # save predicted mask\n","            Image.fromarray(pred_up).save(os.path.join(VIS_DIR, os.path.basename(img_p).replace(\".jpg\",\"_pred.png\")))\n","\n","        except Exception:\n","            print(\"[warn] inference failed for\", img_p)\n","            traceback.print_exc()\n","\n","    # save per-image CSV\n","    save_csv(os.path.join(CSV_DIR, \"per_image_results.csv\"), per_image_rows)\n","\n","    # per-class IoU\n","    class_ids = sorted(list(base_ids | novel_ids))\n","    pci_rows = [[\"class_id\",\"IoU\"]]\n","    per_class_mean = {}\n","    for cid in class_ids:\n","        vals = []\n","        for gt, pr in zip(all_gt, all_pred):\n","            v = compute_class_iou(gt, pr, cid)\n","            if v is not None: vals.append(v)\n","        meanv = float(np.mean(vals)) if vals else 0.0\n","        per_class_mean[cid] = meanv\n","        pci_rows.append([cid, meanv])\n","    save_csv(os.path.join(CSV_DIR, \"per_class_iou.csv\"), pci_rows)\n","    print(\"\\n===== PER-CLASS IoU =====\")\n","    for r in pci_rows:\n","        print(\"\\t\".join(map(str,r)))\n","    print(\"================================\\n\")\n","\n","    # summary\n","    base_vals = [per_class_mean[c] for c in sorted(base_ids) if c in per_class_mean]\n","    novel_vals = [per_class_mean[c] for c in sorted(novel_ids) if c in per_class_mean]\n","    base_score = float(np.mean(base_vals)) if base_vals else 0.0\n","    novel_score = float(np.mean(novel_vals)) if novel_vals else 0.0\n","    mean_score = (base_score + novel_score) / 2.0\n","    summary_rows = [[\"metric\",\"value\"], [\"Base_mIoU\", base_score], [\"Novel_mIoU\", novel_score], [\"Mean_mIoU\", mean_score]]\n","    save_csv(os.path.join(CSV_DIR, \"summary_results.csv\"), summary_rows)\n","    print(\"\\n===== SUMMARY =====\")\n","    for r in summary_rows: print(\"\\t\".join(map(str,r)))\n","    print(\"===================\\n\")\n","\n","    # confusion matrix (pretty print + heatmap)\n","    flat_gt = []\n","    flat_pr = []\n","    for gt, pr in zip(all_gt, all_pred):\n","        flat_gt.extend(gt.flatten().tolist())\n","        flat_pr.extend(pr.flatten().tolist())\n","    if flat_gt:\n","        cm = confusion_matrix(flat_gt, flat_pr, labels=class_ids)\n","        cm_rows = [[\"\"] + class_ids]\n","        for cid, row in zip(class_ids, cm):\n","            cm_rows.append([cid] + row.tolist())\n","        save_csv(os.path.join(CSV_DIR, \"confusion_matrix.csv\"), cm_rows)\n","        pretty_print_confusion(cm, class_ids)\n","        plot_confusion_matrix(cm, class_ids)\n","    else:\n","        print(\"[warn] no pixels available for confusion matrix\")\n","\n","    print(\"\\n=== FINAL ===\")\n","    print(\"CSV folder:\", CSV_DIR)\n","    print(\"Visuals folder:\", VIS_DIR)\n","    print(\"================\\n\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzeuLEWhUxXo","outputId":"65aee220-f68d-48f4-a9a2-55c134ffe897"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[info] extracting zip...\n","[info] extraction done.\n","[info] detected 1 candidate class folders\n","[info] found classes: 1\n","[info] novel classes chosen (names): ['JPEGImages']\n","[info] novel IDs: {1}\n","[info] support size: 5 query size: 17120\n","[info] device = cpu\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 161M/161M [00:00<00:00, 221MB/s]\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["[info] Loaded DeepLabV3\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13.6M/13.6M [00:00<00:00, 182MB/s]\n","Co-occurrence:   0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1269964417.py\", line 198, in compute_cooccurrence\n","    uniq, counts = np.unique(bp[mask_idx], return_counts=True)\n","                             ~~^^^^^^^^^^\n","IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\n","Co-occurrence:  20%|██        | 1/5 [00:00<00:03,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":["[warn] skipping support image due to error: /content/FSS1000/JPEGImages/2011_004869.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1269964417.py\", line 198, in compute_cooccurrence\n","    uniq, counts = np.unique(bp[mask_idx], return_counts=True)\n","                             ~~^^^^^^^^^^\n","IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\n","\rCo-occurrence:  40%|████      | 2/5 [00:01<00:02,  1.48it/s]"]},{"output_type":"stream","name":"stdout","text":["[warn] skipping support image due to error: /content/FSS1000/JPEGImages/2008_001150.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1269964417.py\", line 198, in compute_cooccurrence\n","    uniq, counts = np.unique(bp[mask_idx], return_counts=True)\n","                             ~~^^^^^^^^^^\n","IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\n","\rCo-occurrence:  60%|██████    | 3/5 [00:01<00:01,  1.67it/s]"]},{"output_type":"stream","name":"stdout","text":["[warn] skipping support image due to error: /content/FSS1000/JPEGImages/2010_001337.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1269964417.py\", line 198, in compute_cooccurrence\n","    uniq, counts = np.unique(bp[mask_idx], return_counts=True)\n","                             ~~^^^^^^^^^^\n","IndexError: too many indices for array: array is 2-dimensional, but 3 were indexed\n","\rCo-occurrence:  80%|████████  | 4/5 [00:02<00:00,  1.79it/s]"]},{"output_type":"stream","name":"stdout","text":["[warn] skipping support image due to error: /content/FSS1000/JPEGImages/2012_003636.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-1269964417.py\", line 198, in compute_cooccurrence\n","    uniq, counts = np.unique(bp[mask_idx], return_counts=True)\n","                     }
